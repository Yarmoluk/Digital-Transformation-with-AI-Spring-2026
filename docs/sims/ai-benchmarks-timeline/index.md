---
title: AI Benchmarks Timeline
description: Interactive timeline showing key AI benchmarks and when they were introduced
image: sims/ai-benchmarks-timeline/thumbnail.png
social:
   cards: false
quality_score: 67
hide:
  - toc
---
# AI Benchmarks Timeline

<iframe src="./main.html" height="450px" scrolling="no"></iframe>

Sample iframe:

```html
<iframe src="https://dmccreary.github.io/tracking-ai-course/sims/ai-benchmarks-timeline/main.html"  height="450px" scrolling="no"></iframe>
```

[Run the MicroSim](./main.html){ .md-button .md-button--primary }

[Edit the MicroSim](https://editor.p5js.org/dmccreary/sketches/0fxIbT1A4)


!!! prompt
    I would like you to generate a new p5.js MicroSim sketch that displays a timeline view of the key AI benchmarks and when they were introduced.  Please use the format of the file ai-pace-accelerating.js in the Project knowledge area.  Begin with early benchmarks on simple question answering and finish with the most recent benchmarks that focus on specialized topics like coding skills, math skills, medical diagnosis and answering legal questions to pass a bar exam.

## Self-Assessment Quiz

Test your understanding of AI benchmarks and their evolution.

<div class="upper-alpha" markdown>
**Question 1: What is the primary purpose of AI benchmarks?**

1. To sell AI products to consumers
2. To measure and compare AI system capabilities in a standardized way
3. To limit AI development speed
4. To replace human workers with AI systems
</div>

??? success "Answer"
    B) To measure and compare AI system capabilities in a standardized way - AI benchmarks provide standardized tests that allow researchers and practitioners to objectively compare different AI models and track progress over time.

<div class="upper-alpha" markdown>
**Question 2: How have AI benchmarks evolved over time?**

1. They have become simpler and easier to pass
2. They have remained the same since the 1990s
3. They have progressed from simple question answering to specialized domain tests
4. They have been completely replaced by human evaluation
</div>

??? success "Answer"
    C) They have progressed from simple question answering to specialized domain tests - Early benchmarks focused on basic language understanding, while modern benchmarks test specialized skills like coding, mathematics, medical diagnosis, and legal reasoning.

<div class="upper-alpha" markdown>
**Question 3: Why are specialized benchmarks (like coding or legal reasoning) important?**

1. They are easier to create than general benchmarks
2. They measure AI performance in specific professional domains where AI is being deployed
3. They require less computational power to run
4. They are only used for marketing purposes
</div>

??? success "Answer"
    B) They measure AI performance in specific professional domains where AI is being deployed - As AI systems are increasingly used in professional contexts, specialized benchmarks help evaluate whether AI can perform domain-specific tasks at the level required for real-world applications.

<div class="upper-alpha" markdown>
**Question 4: What does it indicate when AI systems surpass human-level performance on a benchmark?**

1. AI is definitively smarter than humans in all areas
2. The benchmark may no longer be useful for measuring AI progress
3. The benchmark was poorly designed
4. AI development should stop
</div>

??? success "Answer"
    B) The benchmark may no longer be useful for measuring AI progress - When AI surpasses human-level performance on a benchmark, it often indicates that more challenging benchmarks are needed to continue measuring progress in that capability area.

<div class="upper-alpha" markdown>
**Question 5: What is a limitation of using benchmarks to evaluate AI systems?**

1. Benchmarks are too expensive to create
2. Benchmarks may not capture all aspects of real-world performance and can be "gamed"
3. Benchmarks are only available in English
4. Benchmarks cannot measure any useful AI capabilities
</div>

??? success "Answer"
    B) Benchmarks may not capture all aspects of real-world performance and can be "gamed" - While benchmarks provide valuable standardized measurements, they may not fully represent real-world scenarios, and AI systems can sometimes be optimized specifically for benchmark performance without corresponding improvements in practical applications.